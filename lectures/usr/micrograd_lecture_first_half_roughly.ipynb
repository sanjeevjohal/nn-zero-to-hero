{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with a simple function which is a parabola (quadratic)\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)  # start, stop, step. np.arange is like range, but returns a numpy array\n",
    "print(type(xs))\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)  # this gives us a parabola which is concave upx"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Derivatives\n",
    "- now we want to find the slope of the tangent line at a point\n",
    "- it's measuring the rate of change of the function at a point (how fast the function is changing)\n",
    "- rise over run\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# h is the change in x and is very small\n",
    "h = 0.000001\n",
    "x = 2 / 3  # this is optimimum for the parabola i.e. the slope is 0 (local minimum)\n",
    "(f(x + h) - f(x)) / h  # normalised by h to make it a slope"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get more complex, i.e. a function with multiple inputs with a single output d\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "d = a * b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tip\n",
    "The derivative of a function tells you how much the function changes when you make a small change to one of its inputs. In this case, we want to find the derivative of a*b + c with respect to the variable a. This means we want to know how much the value of the function changes when we make a small change to the value of a.\n",
    "\n",
    "The statement says that the derivative of a*b + c with respect to a is simply b. This means that if we change a by a small amount, the value of the function will change by approximately b times that small amount."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can find the derivative of d with respect to a\n",
    "h = 0.0001  # small change\n",
    "\n",
    "# inputs\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d1 = a * b + c  # original value\n",
    "a += h  # change a by a small amount\n",
    "d2 = a * b + c  # new value\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d2 - d1) / h)  # i.e. derivative of d with respect to a is b = -3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Derivatives of a function with multiple inputs\n",
    "- we can use the chain rule to find the derivative of a function with multiple inputs\n",
    "- the derivative of a function with multiple inputs is the sum of the partial derivatives of each input\n",
    "- the partial derivative of a function with respect to a variable is the derivative of the function with respect to that variable, holding all other variables constant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class Features\n",
    "- we want to create a class which can store a value and its gradient\n",
    "- we want to be able to add and multiply values\n",
    "- we want to be able to apply a non-linear activation function\n",
    "- we want to be able to backpropagate the gradient through the graph\n",
    "- we want to be able to visualise the graph\n",
    "- we want to be able to update the value of a variable\n",
    "\n",
    "### Objective\n",
    "- We want to create data structures that's scalable and efficient\n",
    "- Create connecting tissue between the data structures to get lineage (i.e. leave nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    # this is the constructor\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # this will maintain the derivative of L with respect to the node. Initialised to 0 which essentially means it doesn't \"yet\" effect the output value of the loss function\n",
    "        self._backward = lambda: None # it initialises the function to do nothing, and we use a lambda function because it's a one liner\n",
    "        self._prev = set(_children)  # this gives us the lineage of the node ie what nodes are connected to it\n",
    "        self._op = _op  # this tells us the operation that was performed on the node\n",
    "        self.label = label  # this is the label of the node\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    # this is the forward pass by using these private methods (i.e. __add__ and __mul__)\n",
    "    # we use double underscores to make the method private and because the arguments are integers we can use the + and * operators which would have otherwise concatenated strings\n",
    "    # create a backward private method which will be used in the backpropagation algorithm\n",
    "        # use the chain rule: multiply the data of the node by the gradient of the output node\n",
    "        # we return the function, so we don't execute it immediately\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # chain rule: for adding we just propogate as-is\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            # chain rule: we criss-cross the data of the nodes TIMES the gradient of the output node\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):  # tanh is a non-linear activation function\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            # chain rule: we TIMES the data of the node by the gradient of the output node\n",
    "            self.grad += (1 - t ** 2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):  # this is the backpropagation algorithm\n",
    "\n",
    "        topo = []\n",
    "        visited = set()  # set does not allow duplicates and is unordered and unindexed (faster than list)\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b;\n",
    "e.label = 'e'\n",
    "d = e + c;\n",
    "d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f;\n",
    "L.label = 'L'\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def trace(root):\n",
    "    # is a helper function for draw_dot to help build a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()  # set does not allow duplicates and is unordered and unindexed (faster than list)\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name=uid,\n",
    "                 label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad),\n",
    "                 # label=\"{ %s | data %.4f | <b>grad</b> %.4f }\" % (n.label, n.data, n.grad),\n",
    "                 shape='record')\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.grad = 1.0 # derivative of L with respect to L is 1 so can be set manually\n",
    "# likewise the derivative of L with respect to d and f can also be set manually because these are the last hidden layer\n",
    "d.grad = -2\n",
    "f.grad = -4\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ⭐️Explain in simple terms⭐️\n",
    "Now lets see how sensitive are changes in C and E to the output of the loss function**\n",
    "\n",
    "dd / dc = ?\n",
    "\n",
    "- consider this derivative function\n",
    "`(f(x + h) - f(x)) / h i.e. change in f(x) / change in x`\n",
    "- substitute with the function we're interested in\n",
    "`((c + h) + e) - (c + e)) / h`\n",
    "- now expand out\n",
    "`(c + h + e - c - e) / h`\n",
    "- now simplify\n",
    "`(h + 0) / h = 1`\n",
    "\n",
    "## Key takeaways\n",
    "- What this tells us that if we change c by a small amount L will change by the same amount\n",
    "- By symmetry, the same is true for dd / de = 1\n",
    "- NB however these are local (derivative) changes and not global changes which are calculated using the chain rule\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chain Rule\n",
    "\n",
    "- see this [video](https://www.youtube.com/watch?v=IwUJwQYQHkE) for a good explanation of the chain rule or this wikipedia [article](https://en.wikipedia.org/wiki/Chain_rule)\n",
    "    - here's a snippet from the wikipedia article:\n",
    "        - \"Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.\n",
    "        - As put by George F. Simmons: \"if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\"\n",
    "- therefore `dL / dc = (dL / dd) * (dd / dc) = -2 * 1 = -2`\n",
    "- ⭐️ think of any positive operators as simply routing the gradient through the expression graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# therefore we can manually add the gradient to the c and e nodes\n",
    "c.grad += -2\n",
    "e.grad += -2\n",
    "draw_dot(L)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# now lets manually create the gradient for a and b\n",
    "- we know that `dl/de = -2`\n",
    "- we know that the local gradient of `de/da = b` and `de/db = a`\n",
    "- ∴ `dl/da = dl/de * de/da = -2 * -3 = 6`\n",
    "- ∴ `dl/db = dl/de * de/db = -2 * 2 = -4`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# therefore set these gradients manually\n",
    "a.grad += 6\n",
    "b.grad += -4\n",
    "draw_dot(L)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# <editor-fold desc=\"Single optimser step\">\n",
    "# where we want to L to go up by 0.01 ie. in the direction of the gradient,\n",
    "# so we nudge the leaf nodes in the direction of the gradient that we have control over\n",
    "a.data += 0.01 * a.grad\n",
    "b.data += 0.01 * b.grad\n",
    "c.data += 0.01 * c.grad\n",
    "f.data += 0.01 * f.grad\n",
    "\n",
    "# mimic a forward pass\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "\n",
    "print(L.data)\n",
    "# </editor-fold>\n",
    "\n",
    "# # <editor-fold desc=\"reset the data\">\n",
    "# a.data = 2.0\n",
    "# b.data = -3.0\n",
    "# c.data = 10.0\n",
    "# f.data = -2.0\n",
    "# # mimic a forward pass\n",
    "# e = a * b\n",
    "# d = e + c\n",
    "# L = d * f\n",
    "# print(L.data)\n",
    "# # </editor-fold>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just sandpit code to do an inline gradient check FOR ALL THE ABOVE manually created\n",
    "# this is different to numerical gradient which is estimating using small step sizes\n",
    "def lol():\n",
    "    h = 0.001\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a * b\n",
    "    e.label = 'e'\n",
    "    d = e + c\n",
    "    d.label = 'd'\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d * f\n",
    "    L.label = 'L'\n",
    "    L1 = L.data\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    # b.data += h\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a * b\n",
    "    e.label = 'e'\n",
    "    d = e + c\n",
    "    d.label = 'd'\n",
    "    d.data += h # derivative of L with respect to d should be f so can be set manually i.e. -2\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d * f\n",
    "    L.label = 'L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print((L2 - L1) / h)\n",
    "\n",
    "\n",
    "lol()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backpropagation\n",
    "- we want to be able to backpropagate the gradient through the graph i.e. calculate the gradient of the loss with respect to each variable\n",
    "- we're interested in the derivative of the Loss function with respect to the weights of the neural network and need to know how these weights are impacting the loss function\n",
    "- remember only the weights change and iterated on using the gradient descent algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example of a mathematical model of a neuron\n",
    "\n",
    "![Mathematical model of a neuron](./images/Mathematical-model-of-a-biological-neuron.ppm)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Now we're going to backpropagate through a neuron in a neural network (which is a multi-layer perceptron)\n",
    "- Use tanh as the activation function which is a sigmoid function that is zero centered whereas sigmoid is not\n",
    "- the activation function is a squashing function that squashes the output of the neuron to be between -1 and 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)));\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Now let's do a dot.product of the inputs and weights and add the bias\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# deliberately broken down to then be able\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# inputs x1,x2\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m x1 \u001B[38;5;241m=\u001B[39m \u001B[43mValue\u001B[49m(\u001B[38;5;241m2.0\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m x2 \u001B[38;5;241m=\u001B[39m Value(\u001B[38;5;241m0.0\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# weights w1,w2\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Value' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let's do a dot.product of the inputs and weights and add the bias\n",
    "# deliberately broken down to then be able\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron. This number was deliberately chosen\n",
    "b = Value(6.8813735870195432, label='b') # this is the bias that makes the neuron fire when the inputs are 0,0. It's deliberately chosen as 6.8813735870195432 to ...\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1 * w1;\n",
    "x1w1.label = 'x1*w1'\n",
    "x2w2 = x2 * w2;\n",
    "x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2;\n",
    "x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b;\n",
    "n.label = 'n'\n",
    "o = n.tanh(); # tanh is a hyperbolic function that needs more than just pluses and times because you also need exponentiation\n",
    "o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "o._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_dot(o)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SJ: lets fix some gradients\n",
    "o.grad = 1.0\n",
    "# for n, we need to calculate the derivative of the tanh function\n",
    "# do/dn = 1 - tanh(n)^2 = 1 - o^2\n",
    "n_grad = 1 - o.data ** 2 # this is the derivative of the tanh function, see https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh (derivatices section)\n",
    "print(f\"n_grad = {n_grad}\")\n",
    "n.grad = n_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_dot(root=o)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SJ: lets continue the backpropagation\n",
    "- remember the gradient is the derivative of the loss function with respect to the variable\n",
    "- remember a plus is a distributor so we can distribute the gradient to the two inputs i.e. flow back the same gradient to both inputs\n",
    "- for the 1st input layer, for multiplication the local gradient/derivative is the other term\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SJ first lets set the gradient for the 2 intermediate layers where we're adding\n",
    "x1w1x2w2.grad = 0.5\n",
    "b.grad = 0.5\n",
    "x1w1.grad = 0.5\n",
    "x2w2.grad = 0.5\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SJ for the 1st layer, for multiplication it's always the other term so we can do a local chain rule\n",
    "x2.grad = w2.data * x2w2.grad # for x2 it will be (the other term's data w2) * (the gradient of the next layer x2w2)\n",
    "w2.grad = x2.data * x2w2.grad # NB. the derivative tells the sensitivity of the output to this weight w2 which in this case is zero\n",
    "\n",
    "x1.grad = w1.data * x1w1.grad\n",
    "w1.grad = x1.data * x1w1.grad\n",
    "\n",
    "# this completes this MANUAL back propagation and tells us that: (1) w2 has no bearing but w1 does in a positive manner and will be proportionate because the gradient is 1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SJ: Could have simplied this using numpy dot product\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the input vectors x and w as NumPy arrays\n",
    "x = np.array([x1, x2])\n",
    "w = np.array([w1, w2])\n",
    "\n",
    "# Calculate the dot product of x and w using the dot function\n",
    "xw = np.dot(x, w)\n",
    "\n",
    "# xw will contain the result of x1w1 + x2w2\n",
    "draw_dot(xw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SJ: RESUME here\n",
    "- [1:02:13](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "- FORK and push this repo to your own github account"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "\n",
    "\n",
    "def build_topo(v):\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)\n",
    "\n",
    "\n",
    "build_topo(o)\n",
    "topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0 # we manually set this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1w1x2w2._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2w2._backward()\n",
    "x1w1._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.grad = w1.data * x1w1.grad\n",
    "w1.grad = x1.data * x1w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.grad = w2.data * x2w2.grad\n",
    "w2.grad = x2.data * x2w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1w1.grad = 0.5\n",
    "x2w2.grad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1w1x2w2.grad = 0.5\n",
    "b.grad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.grad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - o.data ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = tanh(n)\n",
    "# do/dn = 1 - o**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a;\n",
    "b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b;\n",
    "d.label = 'd'\n",
    "e = a + b;\n",
    "e.label = 'e'\n",
    "f = d * e;\n",
    "f.label = 'f'\n",
    "\n",
    "f.backward()\n",
    "\n",
    "draw_dot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
